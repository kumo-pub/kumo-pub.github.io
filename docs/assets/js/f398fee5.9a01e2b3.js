"use strict";(self.webpackChunkkumo_website=self.webpackChunkkumo_website||[]).push([[4e4],{48446:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var r=t(74848),o=t(28453);const a={title:"The Danger of Atomic Operations",layout:"docs",sidenav:"side-nav-cpp.html",type:"markdown"},i="The Danger of Atomic Operations",s={id:"cpp/mc/atomic_danger",title:"The Danger of Atomic Operations",description:"Dmitry Vyukov, Sanjay Ghemawat, Mike Burrows, Jeffrey Yasskin, Kostya",source:"@site/docs/cpp/mc/atomic_danger.mdx",sourceDirName:"cpp/mc",slug:"/cpp/mc/atomic_danger",permalink:"/docs/next/cpp/mc/atomic_danger",draft:!1,unlisted:!1,tags:[],version:"current",lastUpdatedBy:"Jeff lothar",lastUpdatedAt:1748081772e3,frontMatter:{title:"The Danger of Atomic Operations",layout:"docs",sidenav:"side-nav-cpp.html",type:"markdown"},sidebar:"docs",previous:{title:"\u591a\u6838\u7f16\u7a0b",permalink:"/docs/next/category/\u591a\u6838\u7f16\u7a0b"},next:{title:"log\u65e5\u5fd7\u5e93",permalink:"/docs/next/category/log\u65e5\u5fd7\u5e93"}},c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Existing Components",id:"existing-components",level:2},{value:"Atomic Trickiness",id:"atomic-trickiness",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Testing Considerations",id:"testing-considerations",level:2},{value:"Bug Examples",id:"bug-examples",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"the-danger-of-atomic-operations",children:"The Danger of Atomic Operations"})}),"\n",(0,r.jsx)(n.p,{children:"Dmitry Vyukov, Sanjay Ghemawat, Mike Burrows, Jeffrey Yasskin, Kostya\nSerebryany, Hans Boehm, Ashley Hedberg"}),"\n",(0,r.jsx)(n.p,{children:"First written Apr 22, 2014. Updated Jun 23, 2021."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#introduction",children:"Introduction"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#existing-components",children:"Existing Components"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#atomic-trickiness",children:"Atomic Trickiness"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#performance-considerations",children:"Performance Considerations"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#testing-considerations",children:"Testing Considerations"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"#bug-examples",children:"Bug Examples"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"Most engineers reach for atomic operations in an attempt to produce some\nlock-free mechanism. Furthermore, programmers enjoy the intellectual puzzle of\nusing atomic operations. Both of these lead to clever implementations which are\nalmost always ill-advised and often incorrect. Algorithms involving atomic\noperations are extremely subtle. For example, discovering a general-purpose,\nefficient, lock-free, singly-linked list algorithm took significant research and\nrequired care to implement. Almost all programmers make mistakes when they\nattempt direct use of atomic operations. Even when they don't make mistakes, the\nresulting code is hard for others to maintain."}),"\n",(0,r.jsx)(n.p,{children:"Atomic operations should be used only in a handful of low-level data structures\nwhich are written by a few experts and then reviewed and tested thoroughly.\nUnfortunately, many people attempt to write lock-free code, and this is almost\nalways a mistake. Please do not fall into this trap: do not use atomic\noperations. If you do, you will make mistakes, and those will cost the owners of\nthat code time and money."}),"\n",(0,r.jsx)(n.p,{children:"There are a number of existing higher-level components that are already\ncarefully crafted, reviewed, and tested. Use them if they do what you need.\nOtherwise, use mutexes."}),"\n",(0,r.jsxs)(n.p,{children:["Note: the document is centered around C++, but similar arguments apply to other\nlanguages as well. See ",(0,r.jsx)(n.a,{href:"https://research.swtch.com/mm",children:"research!rsc"})," for a more\ndetailed discussion of hardware, programming language, and Go memory models."]}),"\n",(0,r.jsx)(n.h2,{id:"existing-components",children:"Existing Components"}),"\n",(0,r.jsxs)(n.p,{children:["Reach for commonly-available concurrency components before inventing your own\nsolution using atomics. The list below serves as a guide, but is not exhaustive.\nLibraries such as the ",(0,r.jsx)(n.a,{href:"https://en.cppreference.com/w/cpp",children:"C++ Standard Library"}),",\n",(0,r.jsx)(n.a,{href:"https://abseil.io/",children:"Abseil"}),", and ",(0,r.jsx)(n.a,{href:"https://github.com/facebook/folly",children:"Folly"})," all\ncontain relevant components."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://en.cppreference.com/w/cpp/memory/shared_ptr",children:"std::shared_ptr"})," and\n",(0,r.jsx)(n.a,{href:"https://github.com/facebook/folly/blob/master/folly/synchronization/Hazptr.h",children:"folly's hazard pointer"}),"\nfor reference counting"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://en.cppreference.com/w/cpp/thread/call_once",children:"std::call_once"})," and\n",(0,r.jsx)(n.a,{href:"https://github.com/abseil/abseil-cpp/blob/master/absl/base/call_once.h",children:"absl::call_once"}),"\nfor one-time initialization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://www.boost.org/doc/libs/1_76_0/doc/html/boost_asio/reference/thread_pool.html",children:"boost::asio::thread_pool"}),"\nfor thread pooling"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/abseil/abseil-cpp/blob/master/absl/synchronization/notification.h",children:"absl::Notification"}),"\nfor one-time notifications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://en.cppreference.com/w/cpp/thread/latch",children:"std::latch"}),",\n",(0,r.jsx)(n.a,{href:"https://en.cppreference.com/w/cpp/thread/barrier",children:"std::barrier"}),",\n",(0,r.jsx)(n.a,{href:"https://github.com/abseil/abseil-cpp/blob/master/absl/synchronization/barrier.h",children:"absl::Barrier"}),",\nand\n",(0,r.jsx)(n.a,{href:"https://github.com/abseil/abseil-cpp/blob/master/absl/synchronization/blocking_counter.h",children:"absl::BlockingCounter"}),"\nfor barrier synchronization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://en.cppreference.com/w/cpp/thread/future",children:"std::future"})," for creating\nvalue promises"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/urcu/userspace-rcu/blob/master/doc/cds-api.md",children:"Userspace RCU"}),"\nfor read-copy-update algorithms and lock-free containers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://en.cppreference.com/w/cpp/language/storage_duration",children:"thread_local"}),"\nfor better locality"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/facebook/folly/tree/master/folly/concurrency",children:"folly's concurrency library"}),"\nfor concurrent storage and queues"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/facebook/folly/blob/master/folly/TokenBucket.h",children:"folly::TokenBucket"}),"\nfor rate limiting"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"atomic-trickiness",children:"Atomic Trickiness"}),"\n",(0,r.jsx)(n.p,{children:"Atomic operations introduce two separate kinds of hazards:"}),"\n",(0,r.jsxs)(n.p,{children:["First, unless you exclusively use atomic operations that maintain ordering\nsemantics for all shared memory accesses (notably ",(0,r.jsx)(n.code,{children:"memory_order_seq_cst"}),"\noperations), both compilers and processors can and will visibly reorder memory\naccesses\n",(0,r.jsx)(n.a,{href:"https://en.cppreference.com/w/cpp/atomic/memory_order",children:"per the C++ standard"}),".\nProgramming rules in these cases become far more complicated, and experts often\nstill have trouble pinning them down precisely. Many people find it particularly\nsurprising that such reordering doesn't always stop at traditional\nsynchronization operations, like a mutex acquisition."]}),"\n",(0,r.jsx)(n.p,{children:"If you do restrict yourself to sequentially-consistent operations, you avoid\nthis issue, but may well find that your code now runs slower on ARM and POWER\nthan if you had used mutexes. ARM and POWER are weakly-ordered systems, so\nspecial CPU load instructions or memory fences are required to achieve\nsequential consistency. This is not required on strongly-ordered platforms like\nx86."}),"\n",(0,r.jsxs)(n.p,{children:["Second, it's extremely difficult to write code in a world in which a) only\nindividual memory accesses are atomic, and b) no way to achieve mutual exclusion\nover larger code sections exists. Object lifetime management is difficult in a\nconcurrent setting. ",(0,r.jsx)(n.a,{href:"http://en.wikipedia.org/wiki/Compare-and-swap",children:"CAS-based"}),"\nalgorithms are subject to the\n",(0,r.jsx)(n.a,{href:"http://en.wikipedia.org/wiki/ABA_problem",children:"ABA problem"}),". Unexpected and\nunreproducible thread interleavings occur. Sequences of atomic operations are\nthen not atomic as a whole. Before approaching atomic operations, you must be\nready for all of these problems and understand the language memory model with\nrespect to ordering, atomicity, visibility, and data races."]}),"\n",(0,r.jsx)(n.p,{children:"Don't assume x86 semantics. Hardware platform guarantees matter only if you are\nprogramming in assembly. Higher-level language (C++/Java/Go) compilers can break\nyour code. Furthermore, ARM and POWER provide notably different and more complex\nmemory models; these can also break your code if you run on a variety of\nhardware."}),"\n",(0,r.jsx)(n.p,{children:"Let's consider two examples based on real code that demonstrate these two kinds\nof subtleties related to atomic operations. First example:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-c++",children:"std::atomic<bool> data_ready = false;\ndouble data = 0.0;\n\nvoid Thread1() {\n  data = 1.23;\n  data_ready.store(true, std::memory_order_relaxed);\n}\n\nvoid Thread2() {\n  if (data_ready.load(std::memory_order_relaxed))\n    CHECK(data == 1.23);\n}\n"})}),"\n",(0,r.jsx)(n.p,{children:"The code is seemingly correct: Thread1 initializes the data first and then sets\nthe flag, Thread2 ensures that the flag is set and only then reads the data.\nWhat can possibly go wrong?"}),"\n",(0,r.jsx)(n.p,{children:"With optimizations enabled, gcc compiles this code to:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"% g++ -O2 test.cc -S && cat test.s\n\nThread1:\n  movabsq $4608218246714312622, %rax # 1. Load the constant into RAX\n  movl    $1, data_ready(%rip)       # 2. Store 1 into data_ready\n  movq    %rax, data(%rip)           # 3. Store RAX register into data\n  ret\n"})}),"\n",(0,r.jsxs)(n.p,{children:["If Thread2 is executed between instructions 2 and 3 of Thread1, the ",(0,r.jsx)(n.code,{children:"CHECK"})," in\nThread2 will fail. Note that the compiler does exactly what we asked it to do.\nThe operations on ",(0,r.jsx)(n.code,{children:"data_ready"})," are indeed atomic; they are just reordered with\nother memory accesses."]}),"\n",(0,r.jsxs)(n.p,{children:["Another example, this time with implicit ",(0,r.jsx)(n.code,{children:"memory_order_seq_cst"}),". Here, we have a\nconcurrent object pool based on a lock-free stack, whose algorithm tries to work\naround the ABA problem in a non-traditional way:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-c++",children:"template<typename T>\nclass ConcurrentPool {\n public:\n  ConcurrentPool(size_t size)\n      : head_(0),\n       size_(size),\n       array_(new Node[size]) {\n    for (size_t i = 0; i < size; i++)\n      array_[i].next.store(i + 1);\n    array_[size - 1].next.store(kEnd);\n  }\n\n  T* Get() {\n    while (size_.load() > 1) {\n      size_t head1 = head_.load();\n      size_t next1 = array_[head1].next.exchange(kEnd);\n      if (next1 != kEnd) {\n        if (head_.compare_exchange_strong(head1, next1)) {\n          size_.fetch_sub(1);\n          return &array_[head1].v;\n        } else {\n          array_[head1].next.exchange(next1);\n        }\n      }\n    }\n    return nullptr;\n  }\n\n  void Put(T* v) {\n    Node *n = reinterpret_cast<Node*>(v);\n    size_t i = n - &array_[0];\n    size_t head1;\n    do {\n      head1 = head_.load();\n      n->next.store(head1);\n    } while (!head_.compare_exchange_strong(head1, i));\n    size_.fetch_add(1);\n  }\n\n private:\n  struct Node {\n    T v;\n    atomic<size_t> next;\n  };\n\n  atomic<size_t> head_;\n  atomic<size_t> size_;\n  unique_ptr<Node[]> array_;\n\n  static const size_t kEnd = -1;\n};\n"})}),"\n",(0,r.jsx)(n.p,{children:"Before reading further try to spot the bug in this code."}),"\n",(0,r.jsx)(n.p,{children:"The bug is basically impossible to discover by testing and manual code\ninspection. It was found by an automatic checker of synchronization algorithms.\nThe particular execution that leads to the bug:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Thread 1 reads ",(0,r.jsx)(n.code,{children:"head_ = 0"})," in ",(0,r.jsx)(n.code,{children:"Get()"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Thread 0 reads ",(0,r.jsx)(n.code,{children:"head_ = 0"})," in ",(0,r.jsx)(n.code,{children:"Get()"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Thread 0 removes element 0 from the stack, ",(0,r.jsx)(n.code,{children:"now head_ = 1"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"Thread 0 starts putting the element 0."}),"\n",(0,r.jsxs)(n.li,{children:["Thread 0 reads ",(0,r.jsx)(n.code,{children:"head_ = 1"}),", and sets the next field of the element 0 to 1."]}),"\n",(0,r.jsxs)(n.li,{children:["Thread 1 executes ",(0,r.jsx)(n.code,{children:"exchange"})," on the next field of the element 0. It reads 1\nand writes -1."]}),"\n",(0,r.jsxs)(n.li,{children:["Thread 2 gets the element 1 from the stack, now ",(0,r.jsx)(n.code,{children:"head_ = 2"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Thread 0 fails with ",(0,r.jsx)(n.code,{children:"compare_exchange"})," in ",(0,r.jsx)(n.code,{children:"Put()"}),", re-reads ",(0,r.jsx)(n.code,{children:"head_ = 2"}),", and\nwrites 2 to the next field of the element 0."]}),"\n",(0,r.jsxs)(n.li,{children:["Thread 0 succeeds with ",(0,r.jsx)(n.code,{children:"compare_exchange"})," in ",(0,r.jsx)(n.code,{children:"Put()"}),". Now ",(0,r.jsx)(n.code,{children:"head_ = 0"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Thread 1 succeeds with ",(0,r.jsx)(n.code,{children:"compare_exchange"})," in ",(0,r.jsx)(n.code,{children:"Get()"}),". Now ",(0,r.jsx)(n.code,{children:"head_ = 1"}),"\n(however ",(0,r.jsx)(n.code,{children:"head_"})," must be equal to 2!)."]}),"\n",(0,r.jsx)(n.li,{children:"Thread 0 pops element 1 from the stack."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Now both threads 0 and 2 work with the element 1. Bang!"}),"\n",(0,r.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,r.jsx)(n.p,{children:"Programmers assume that mutexes are expensive, and that using atomic operations\nwill be more efficient. But in reality, acquiring and releasing a mutex is\ncheaper than a cache miss; attention to cache behavior is usually a more\nfruitful way to improve performance. Furthermore, lock-free data structures are\noften more expensive than using mutexes. A mutex allows arbitrary changes to be\nmade to a complex data structure; if the same changes must be made without a\nmutex, the result is likely to take more atomic read-modify-write and memory\nfence instructions, not fewer."}),"\n",(0,r.jsx)(n.p,{children:"People wish to avoid mutex contention when concurrency is high. Reducing\nconcurrency is best solved by partitioning locked data structures to avoid mutex\ncontention. For example, it is easier, more efficient, and more useful to build\na high-concurrency hash table from many normal hash tables, each with its own\nreader-writer mutex, than to build one lock-free hash table using atomic\noperations."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://en.cppreference.com/w/cpp/language/storage_duration",children:"Thread-local"}),"\ncaching and batching of updates of centralized state is another technique that\nusually vastly outperforms centralized lock-free algorithms. For example,\n",(0,r.jsx)(n.a,{href:"https://github.com/google/tcmalloc",children:"tcmalloc"})," uses it to achieve outstanding\nscaling while relying only on mutexes for synchronization."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Reference_counting",children:"Reference-counting"})," can help\nto significantly reduce the size of critical sections in some scenarios. Namely,\nread-lock a container, find the necessary object, increment reference counter,\nunlock, and return:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-c++",children:"V *find(T key) {\n  lock_guard l(mutex);\n  V *v = container.find(key);\n  if (v != nullptr)\n    v->refcount.Acquire();\n  return v;\n  // Work with the object v happens outside of the mutex.\n  // Caller calls v->refcount.Release() when done with the object.\n}\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The\n",(0,r.jsx)(n.a,{href:"http://en.wikipedia.org/wiki/Multiversion_concurrency_control",children:"Read-Copy-Update/Multiversion-Concurrency-Control"}),"\ntechnique allows one to achieve linear scaling for read-mostly data structures."]}),"\n",(0,r.jsx)(n.h2,{id:"testing-considerations",children:"Testing Considerations"}),"\n",(0,r.jsx)(n.p,{children:"Unit tests do not provide good enough coverage for lock-free algorithms; they\nexplore a negligible part of all possible thread interleavings. For a small\nsynchronization algorithm with N=10 atomic operations and T=4 threads, the total\nnumber of possible thread interleavings is O(T^(T*N)) ~= 10^24. Memory order\nrelaxations result in an even larger number of potential executions. Unit tests\nwill cover a thousand executions at best."}),"\n",(0,r.jsx)(n.p,{children:"Moreover, x86 hardware can't yield all executions possible on POWER and ARM\nplatforms. Code compiled with a particular version of compiler and flags may not\nbe able to yield executions possible with a different compiler or flags. Future\ncompilers are likely to more aggressively reorder memory accesses than current\ncompilers."}),"\n",(0,r.jsx)(n.p,{children:"The human brain is poor at reasoning about concurrent algorithms that are not\nsequentially consistent. Any non-trivial lock-free algorithm requires careful\nreview by several experts, verification with formal checkers, and exhaustive\nstress testing on different hardware at a minimum."}),"\n",(0,r.jsxs)(n.p,{children:["Note that even mutex-based algorithms can be complex (or a lock can be simply\nforgotten). Use ",(0,r.jsx)(n.a,{href:"https://github.com/google/sanitizers",children:"ThreadSanitizer"})," to test\nfor data races and certain kinds of deadlocks."]}),"\n",(0,r.jsx)(n.h2,{id:"bug-examples",children:"Bug Examples"}),"\n",(0,r.jsx)(n.p,{children:"Here are examples of several bugs in algorithms based on atomic operations. The\nbugs are harmful, tricky, and were lurking in our codebases for years."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Linux kernel lock-free fd lookup"})}),"\n",(0,r.jsxs)(n.p,{children:["The bug was introduced on\n",(0,r.jsx)(n.a,{href:"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=ab2af1f5005069321c5d130f09cce577b03f43ef",children:"Sep 9, 2005"}),"\nas part of a migration from a spinlock to RCU refcounting. The change introduced\na bug in how the code needs to react on a narrow window of semi-inconsistent\nstate exposed by concurrent updates. It was fixed ten years later, on\n",(0,r.jsx)(n.a,{href:"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=5ba97d2832f8",children:"Jul 1, 2015"}),"."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Data Plane Development Kit's RTE Ring"})}),"\n",(0,r.jsxs)(n.p,{children:["The bug existed in the first public release of DPDK, which was on\n",(0,r.jsx)(n.a,{href:"http://git.dpdk.org/dpdk/commit/?id=af75078fece3",children:"Mar 11, 2013"}),". There was a\nbug with issuing a zero objects dequeue with multiple consumers. It was possible\nto get more than one thread to succeed the compare-and-set operation and observe\nstarvation or even deadlock in the while loop that checks for preceding\ndequeues. The same was possible on the enqueue path. The bug was fixed on\n",(0,r.jsx)(n.a,{href:"http://git.dpdk.org/dpdk/commit/?id=d0979646166e740917baaabc4b78ded3482226b7",children:"Mar 22, 2016"}),"."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"sync.WaitGroup"})}),"\n",(0,r.jsxs)(n.p,{children:["The bug was introduced on\n",(0,r.jsx)(n.a,{href:"https://github.com/golang/go/commit/ee6e1a3ff77",children:"Jul 18, 2011"})," as part of a\nWaitGroup rewrite that was intended to improve scalability. The change indeed\nimproved performance and scalability, but it also replaced a simple mutex-based\nalgorithm with a trickier one based on atomic operations. The bug occurred in\nvery rare circumstances but led to arbitrary memory corruptions. It was\ndiscovered and fixed only on\n",(0,r.jsx)(n.a,{href:"https://github.com/golang/go/commit/e9347c781be",children:"Apr 10, 2014"}),". The bug was\ncaused by an unexpected thread interleaving."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Parallel GC"})}),"\n",(0,r.jsxs)(n.p,{children:["The bug was introduced on\n",(0,r.jsx)(n.a,{href:"https://github.com/golang/go/commit/d324f2143b2",children:"Sep 30, 2011"})," and fixed only\non ",(0,r.jsx)(n.a,{href:"https://github.com/golang/go/commit/b3a3afc9b78",children:"Jan 15, 2014"}),". The bug led\nto arbitrary memory corruptions on overloaded machines. The bug was due to\nunexpected thread interleaving."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"org.jctools.maps.NonBlockingHashMap"})}),"\n",(0,r.jsxs)(n.p,{children:["The bug was introduced sometime before\n",(0,r.jsx)(n.a,{href:"https://twitter.com/nitsanw/status/1406871256486580229",children:"Feb 2009"}),". The bug\nallowed the remove operation to return the same item more than once. The root\ncause was an inconsistency between a failed CAS and a subsequent atomic read of\nthe same field. It was identified on\n",(0,r.jsx)(n.a,{href:"https://github.com/JCTools/JCTools/issues/205#",children:"Jan 15, 2018"})," and fixed on\n",(0,r.jsx)(n.a,{href:"https://github.com/JCTools/JCTools/commit/69786bb178f194b7dad5e4dbf84bed41db5af94e",children:"Jan 21, 2018"}),"\nafter much discussion."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>s});var r=t(96540);const o={},a=r.createContext(o);function i(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);